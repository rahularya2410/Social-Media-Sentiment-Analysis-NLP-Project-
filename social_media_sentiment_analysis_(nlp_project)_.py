# -*- coding: utf-8 -*-
"""Social Media Sentiment Analysis (NLP Project) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vUfSO9pN8L_VKihf3FVcGwgzML3kMMtT

#Social Media Sentiment Analysis (NLP Project)

Short description of DataSet

The dataset used in this project consists of Reddit posts related to artists.
Each row represents a unique social media post written by a Reddit user.
The key feature of the dataset is the text column, which contains public opinions about artworks and artists.
The dataset allows understanding of how users feel and react toward artistic content by analyzing posts classified into Positive, Negative, and Neutral sentiment categories.
"""

# Install libraries
!pip install vaderSentiment
import pandas as pd
import matplotlib.pyplot as pt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import nltk

# Download NLP resources (only first time)
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("punkt_tab")

#Load Dataset
df = pd.read_csv("reddit_artist_posts_sentiment.csv")
print("Dataset Loaded Successfully")
df.head()

# Assuming the column containing text is named "text" or "post"
TEXT_COL = "text" if "text" in df.columns else "post"

print(df.info())

""" Text Preprocessing"""

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = word_tokenize(str(text).lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return " ".join(tokens)

df["clean_text"] = df[TEXT_COL].apply(preprocess)

"""Sentiment Classification (VADER)"""

analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    score = analyzer.polarity_scores(text)["compound"]
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

df["Sentiment"] = df["clean_text"].apply(get_sentiment)

print("\nSentiment Count:")
print(df["Sentiment"].value_counts())

"""Visualization 1 – Sentiment Distribution"""

pt.figure(figsize=(6,4))
sns.countplot(x=df["Sentiment"], palette="Set2")
pt.title("Sentiment Distribution")
pt.xlabel("Sentiment")
pt.ylabel("Count")
pt.show()

"""Visualization 2 – Word Cloud for each sentiment"""

for sentiment in ["Positive", "Neutral", "Negative"]:
    text = " ".join(df[df["Sentiment"] == sentiment]["clean_text"])
    wordcloud = WordCloud(width=900, height=500, background_color="white").generate(text)
    pt.figure(figsize=(8,5))
    pt.imshow(wordcloud, interpolation="bilinear")
    pt.axis("off")
    pt.title(f"Word Cloud - {sentiment} Posts")
    pt.show()

"""Visualization 3 – Sentiment Over Time"""

# Safe Date Column Detection for Trend Graph

DATE_COL = None

for col in df.columns:
    # Check if column name looks like a date
    if "date" in col.lower() or "time" in col.lower() or "utc" in col.lower():
        try:
            # Try converting a small sample row to datetime
            pd.to_datetime(df[col].iloc[0])
            DATE_COL = col
            break
        except:
            continue

if DATE_COL:
    print(f"\n Date column detected automatically: {DATE_COL}")
    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors="coerce")
    df = df.dropna(subset=[DATE_COL])
    df["date_only"] = df[DATE_COL].dt.date

    trend_df = df.groupby(["date_only", "Sentiment"]).size().reset_index(name="Count")

    plt.figure(figsize=(10,5))
    sns.lineplot(data=trend_df, x="date_only", y="Count", hue="Sentiment")
    plt.xticks(rotation=45)
    plt.title("Sentiment Trend Over Time")
    plt.show()

else:
    print("\nNo valid date/time column detected — trend graph skipped.")

"""Top 20 Keywords by Sentiment"""

if "subreddit" in df.columns:
    plt.figure(figsize=(12,5))
    sns.countplot(data=df, x="subreddit", hue="Sentiment")
    plt.title("Sentiment per Subreddit")
    plt.xticks(rotation=45)
    plt.show()
else:
    print("No 'subreddit' column found — skipping subreddit graph.")

from collections import Counter

for sentiment in ["Positive", "Neutral", "Negative"]:
    text = " ".join(df[df["Sentiment"] == sentiment]["clean_text"])
    word_freq = Counter(text.split()).most_common(20)
    words, counts = zip(*word_freq)

    pt.figure(figsize=(8,4))
    sns.barplot(x=list(counts), y=list(words), palette="Paired")
    pt.title(f"Top 20 Keywords in {sentiment} Posts")
    pt.xlabel("Frequency")
    pt.ylabel("Words")
    pt.show()

# Post Length vs Sentiment
df["post_length"] = df["clean_text"].apply(lambda x: len(x.split()))

pt.figure(figsize=(7,5))
sns.boxplot(data=df, x="Sentiment", y="post_length", palette="coolwarm")
pt.title("Post Length vs Sentiment")
pt.xlabel("Sentiment")
pt.ylabel("Number of Words in Post")
pt.show()

# Model Evaluation – Confusion Matrix & Classification Scores
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# True & Predicted labels (same here because VADER makes direct predictions)
y_true = df["Sentiment"]
y_pred = df["Sentiment"]

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred, labels=["Negative", "Neutral", "Positive"])

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative", "Neutral", "Positive"],
            yticklabels=["Negative", "Neutral", "Positive"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Classification Report (Precision, Recall, F1)
print("\nClassification Report:")
print(classification_report(y_true, y_pred, digits=3))

# Accuracy Score
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy Score:", round(accuracy * 100, 2), "%")

#Export Final Result

df.to_csv("Sentiment_Analysis_Final_Output.csv", index=False)
print("\nSentiment Analysis Output Exported Successfully!")